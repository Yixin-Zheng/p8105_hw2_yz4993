---
title: "p8105_hw2_yz4993"
author: "Yixin Zheng"
date: "2024-10-02"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(dplyr)
library(knitr)
```

# Problem 1

Below we import and clean data from `NYC_Transit_Subway_Entrance_And_Exit_Data.csv`. The process begins with data import, updates variable names, and selects the columns that will be used in later parts fo this problem. We update `entry` from `yes` / `no` to a logical variable. As part of data import, we specify that `Route` columns 8-11 should be character for consistency with 1-7.

```{r 1.1 data import and cleaning}
trans_ent = 
  read_csv(
    "data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
    col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) |> 
  janitor::clean_names() |> 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

As it stands, these data are not "tidy": route number should be a variable, as should route. That is, to obtain a tidy dataset we would need to convert `route` variables from wide to long format. This will be useful when focusing on specific routes, but may not be necessary when considering questions that focus on station-level variables.

The following code chunk selects station name and line, and then uses `distinct()` to obtain all unique combinations. As a result, the number of rows in this dataset is the number of unique stations. .

```{r 1.2 distinct stations number}
trans_ent |> 
  select(station_name, line) |> 
  distinct()
```

The next code chunk is similar, but filters according to ADA compliance as an initial step. This produces a dataframe in which the number of rows is the number of ADA compliant stations.

```{r 1.3 ada compliant station number}
trans_ent |> 
  filter(ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

To compute the proportion of station entrances / exits without vending allow entrance, we first exclude station entrances that do not allow vending. Then, we focus on the `entry` variable -- this logical, so taking the mean will produce the desired proportion (recall that R will coerce logical to numeric in cases like this).

```{r 1.4 proportion of station w/o v.a.e}
trans_ent |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```

Lastly, we write a code chunk to identify stations that serve the A train, and to assess how many of these are ADA compliant. As a first step, we tidy the data as alluded to previously; that is, we convert `route` from wide to long format. After this step, we can use tools from previous parts of the question (filtering to focus on the A train, and on ADA compliance; selecting and using `distinct` to obtain dataframes with the required stations in rows).

```{r 1.5 distinct station A train + ada}
trans_ent |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A") |> 
  select(station_name, line) |> 
  distinct()

trans_ent |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

# Problem 2

```{r 2.1 data import and cleaning/ mr. trash wheel}
mr_trash_wheel = 
  read_excel(
    "data/202409 Trash Wheel Collection Data.xlsx",
    sheet = "Mr. Trash Wheel",      # specify the correct sheet
    skip = 1,                       # skip first rows (image)
    .name_repair = "minimal"        # suppress new names message
  ) |> 
  janitor::clean_names() |>         # clean column names to snake_case
  select(1:14) |>                   # select only the first 14 columns
  filter(!is.na(dumpster)) |>       # omit rows without dumpster-specific data
  mutate(
    year = as.double(year),                          # change variable type to num
    sports_balls = as.integer(round(sports_balls)),  # round and change to integer
    wheel_name = "mr.trash wheel"                    # add a variable for wheel
  )
```

```{r 2.2 data import and cleaning/ prof trash wheel}
prof_trash_wheel = 
  read_excel(
    "data/202409 Trash Wheel Collection Data.xlsx",
    sheet = "Professor Trash Wheel",     # specify the correct sheet
    skip = 1,                       # skip first rows (image)
    .name_repair = "minimal"        # suppress new names message
  ) |> 
  janitor::clean_names() |>         # clean column names to snake_case
  select(1:13) |>                   # select only the first 13 columns
  filter(!is.na(dumpster)) |>       # omit rows without dumpster-specific data
  mutate(
    sports_balls = NA_integer_,     # add missing sports_balls column (integer)
    wheel_name = "professor trash wheel"             # add a variable for wheel
  )
```

```{r 2.3 data import and cleaning/ gwynnda trash wheel}
gwyn_trash_wheel = 
  read_excel(
    "data/202409 Trash Wheel Collection Data.xlsx",
    sheet = "Gwynnda Trash Wheel",     # specify the correct sheet
    skip = 1,                       # skip first rows (image)
    .name_repair = "minimal"        # suppress new names message
  ) |> 
  janitor::clean_names() |>         # clean column names to snake_case
  select(1:12) |>                   # select only the first 12 columns
  filter(!is.na(dumpster)) |>       # omit rows without dumpster-specific data
  mutate(
    sports_balls = NA_integer_,    # add missing sports_balls column (integer)
    glass_bottles = NA_real_,        # add missing glass_bottles column (number)
    wheel_name = "gwynnda trash wheel"               # add a variable for wheel
  )
```

```{r 2.4 combine datasets}
trash_wheel = bind_rows(mr_trash_wheel, prof_trash_wheel, gwyn_trash_wheel)
names(trash_wheel)
```

The combined trash_wheel dataset contains a total of `r nrow(trash_wheel)` observations and `r ncol(trash_wheel)` variables, which are: `r paste(names(trash_wheel), collapse = ", ")`.

During data cleaning, for the `mr_trash_wheel` data sheet, we converted the variable type of `r colnames(trash_wheel)[3]` from character to numeric to ensure the dataset could be combined. Additionally, the `r colnames(trash_wheel)[13]` variable was rounded and converted to an integer as per the instructions.

Both the `prof_trash_wheel` and `gwyn_trash_wheel` datasets were missing the `r colnames(trash_wheel)[13]` column, and the `gwyn_trash_wheel` dataset was also missing the `r colnames(trash_wheel)[10]` column. To maintain consistency in the format of the dataset, these missing columns were added with NA values, enabling the use of the `bind_rows()` function for combining the datasets.

We also added a new variable, `wheel_name`, to each dataset to identify the specific trash wheel. Key variables include `r colnames(trash_wheel)[5]`, which indicates the weight of trash collected (in tons), and `r colnames(trash_wheel)[6]`, which measures the volume of trash collected (in cubic yards) for each corresponding dumpster. The variables `r paste(colnames(trash_wheel)[7:13], collapse = ", ")` represent different types of trash collected, such as plastic bottles and cigarette butts. Finally, the variable `r colnames(trash_wheel)[14]` denotes the number of homes powered by the electricity generated from the collected trash, where each ton of trash equates to an average of 500 kilowatts of electricity, and an average household uses approximately 30 kilowatts per day.

```{r 2.5 calculate}
# total weight of trash collected by Professor Trash Wheel
prof_total_weight <- trash_wheel %>%
  filter(wheel_name == "professor trash wheel") %>%
  pull(weight_tons) %>%
  sum(na.rm = TRUE)

# total number of cigarette butts collected by Gwynnda in June of 2022
gwyn_2022_6_cigarette_butts <- trash_wheel %>%
  filter(wheel_name == "gwynnda trash wheel", month == "June", year == 2022) %>%
  pull(cigarette_butts) %>%
  sum(na.rm = TRUE)

prof_total_weight
gwyn_2022_6_cigarette_butts
```

As the code result shown, the total weight of trash collected by Professor Trash Wheel is 246.74 tons, and the total number of cigarette butts collected by Gwynnda in June of 2022 is 18120.

# Problem 3

```{r 3.1 data import and cleaning and wrangling}
bakers = 
  read_csv(
    "data/gbb_datasets/bakers.csv"    # specify the correct path
  ) |> 
  janitor::clean_names() |>           # clean column names to snake_case
  filter(!is.na(baker_name)) |>       # filter "baker_name" column to omit NA
  separate(
    baker_name, 
    into = c("first_name", "last_name"), 
    sep = " ", 
    extra = "merge"                   # split first and last names
  ) |> 
  mutate(
    first_name = ifelse(first_name == "Jo", "Joanne", first_name) # convert Jo to Joanne
  )

bakes = 
  read_csv(
    "data/gbb_datasets/bakes.csv"     # specify the correct path
  ) |> 
  janitor::clean_names() |>            # clean column names to snake_case
  mutate(
    baker = gsub('^"|"$', '', baker),   # remove extra quotes from baker names
    baker = ifelse(baker == "Jo", "Joanne", baker)  # convert Jo to Joanne
  )

results = 
  read_csv(
    "data/gbb_datasets/results.csv",    # specify the correct path
    skip = 2) |>                        # skip the first 2 rows (notes)
  janitor::clean_names() |>             # clean column names to snake_case
  filter(!is.na(baker))                 # filter rows "baker" column to omit NA
```

```{r 3.2 completeness and correctness}
# checking for missing values
sum(is.na(bakers))
sum(is.na(bakes))
sum(is.na(results))

#review column names
colnames(bakers)
colnames(bakes)
colnames(results)

# check for bakers in bakes.csv that do not exist in bakers.csv based on first name
aj_bebr = anti_join(bakes, bakers, by = c("baker" = "first_name"))
# check for bakers in bakers.csv that do not exist in bakes.csv based on first name
aj_brbe = anti_join(bakers, bakes, by = c("first_name" = "baker"))
# check for bakers in bakes.csv that do not exist in results.csv
aj_ber = anti_join(bakes, results, by = c("baker" = "baker"))
# check for bakers in results.csv that do not exist in bakes.csv
aj_rbe = anti_join(results, bakes, by = c("baker" = "baker"))
# check for bakers in bakers.csv that do not exist in results.csv based on first name
aj_brr = anti_join(bakers, results, by = c("first_name" = "baker"))
# check for bakers in results.csv that do not exist in bakers.csv based on first name
aj_rbr = anti_join(results, bakers, by = c("baker" = "first_name"))


# check for replicated first_name in the aj_brbe (bakers vs bakes)
replicated_brbe <- aj_brbe %>%
  group_by(first_name) %>%
  filter(n() > 1)

# check for replicated baker name in the aj_rbe (results vs bakes)
replicated_rbe <- aj_rbe %>%
  group_by(baker) %>%
  filter(n() > 1)

# combine the replicated entries from aj_brbe
combined_brbe <- aj_brbe %>%
  group_by(first_name) %>%
  summarize(
    last_name = paste(unique(last_name), collapse = "/"),
    series = paste(unique(series), collapse = ", "),
    baker_age = mean(baker_age, na.rm = TRUE)
  )

# combine the replicated entries from aj_rbe
combined_rbe <- aj_rbe %>%
  group_by(baker) %>%
  summarize(
    episode = paste(unique(episode), collapse = ", "),
    result = paste(unique(result), collapse = "/")
  )

# Check if the names in combined_brbe and combined_rbe match
if (all(combined_brbe$first_name %in% combined_rbe$baker) &&
    all(combined_rbe$first_name == combined_rbe$baker)) {
  print("All names match.")
} else {
  print("Some names differ.")
}
```

Firstly, I imported the datasets and cleaned the column names using a similar approach as in previous problems: specifying the correct path, cleaning the column names to `snake_case`, and filtering out any null values in the key identifier columns (`baker_name`/`baker`). Additionally, I skipped the first two rows in `results.csv` since these rows contain notes, not data. During the next step, when using the `anti_join` function, I noticed that each `anti_join` returned non-empty tibbles. After closely examining the datasets one by one, we found that the baker "Jo" in both `bakers.csv` and `bakes.csv` was the same person as "Joanne" in `results.csv`. To ensure consistency, I updated the data cleaning process to convert "Jo" to "Joanne" in both the `bakers` and `bakes` datasets.

Afterward, only the tibbles generated from lines 219 and 223 remained non-empty. Upon further inspection, it became clear that these tibbles contained replicated entries. To handle this, I identified and processed the replicated entries in both the `bakers` vs. `bakes` and `results` vs. `bakes` datasets. I first checked for replicated `first_name` values in `bakers` and replicated `baker` names in `results`. Then, I combined the duplicated entries and compared the names from both datasets. The result showed that all names matched, indicating that the 21 bakers, although present in both `bakers.csv` and `results.csv`, lacked data on their Signature Bakes. Since these bakers still participated in the competition, we chose not to omit them.

Regarding data correctness, it is likely that the data entry was done with minimal caution, necessitating a significant amount of cleaning before further analysis to avoid bias. As for data completeness, the `sum(is.na())` function helped identify missing values: `bakers.csv` had no missing values, suggesting good completeness; `bakes.csv` had only 2 missing values, which is acceptable; whereas `results.csv` had a considerable number of missing values (866), raising concerns about its completeness.

We will now proceed to merge the datasets into a single, final dataset and organize it by merge datasets on common keys, and rearranging variables. As we mentioned previously, since some bakers are missing bake-related information but are present in the competition (as seen with the replicated entries), I would keep these rows for completeness. However, in the future, if needed, we should flag missing data or fill them with appropriate NA values to avoid bias in analysis.

```{r 3.3 merge and organize}
# merging datasets on 'first_name' and 'series' keys
final_data <- bakers |> 
  left_join(bakes, by = c("first_name" = "baker", "series")) |> 
  left_join(results, by = c("first_name" = "baker", "series", "episode"))

# rearrange columns
final_data <- final_data %>%
  select(
    first_name, last_name, baker_age, baker_occupation, hometown,  # baker info
    series, episode, signature_bake, show_stopper,                 # bake info
    technical, result                                              # results
  )

# sorting by 'series' and 'episode'
final_data <- final_data %>%
  arrange(series, episode)

# display the final dataset
print(final_data)

# save the final dataset to a CSV file
write_csv(final_data, "data/gbb_datasets/final_bake_data.csv")
```

The final dataset contains `r nrow(final_data)` observations and `r ncol(final_data)` variables, which include the following: `r paste(names(final_data), collapse = ", ")`. During the data cleaning process, the baker name discrepancies, such as "Joanne" being converted to "Jo", were corrected to ensure consistency across datasets. Additionally, `bakes.csv` had 21 bakers not present in `bakers.csv` or `results.csv`, especially because of missing values in columns like `signature_bake`, which likely indicate that these bakers participated but the data was not recorded. Notably, the variables such as `r colnames(final_data)[6]` for the series, `r colnames(final_data)[7]` for the episode, and the technical performance details, including `r colnames(final_data)[10]` for the technical score and `r colnames(final_data)[11]` for the result, are important for evaluating the bakers' performance throughout this competition. This cleaned and merged dataset can now be used for further analysis.

```{r 3.4 create table}
star_baker <- final_data %>%
  filter(
    result %in% c("STAR BAKER", "WINNER"), 
    series >= 5 & series <= 10) %>%
  select(series, episode, first_name, last_name, result) %>%
  arrange(series, episode)

# Create a reader-friendly table
kable(star_baker, caption = "Star Baker and Winner of Each Episode (Seasons 5-10)")
```

This table shows the "Star Baker" for each episode and "Winner" for each seasons from seasons 5-10, highlighting the unpredictability of the final outcomes. For instance, Richard Burr (Season 5) won "Star Baker" five times but did not win the season. Nadiya Hussain in Season 6, who had the most "Star Baker" wins, did go on to win her season. Meanwhile, Sophie Faldo in Season 8 only won "Star Baker" twice but still emerged as the overall winner.

```{r 3.5 viewers data}
# import, clean, tidy, and organize the viewership data inÂ viewers.csv. 
viewers <- read_csv("data/gbb_datasets/viewers.csv") |>  # specify the correct path
  janitor::clean_names() |>                              # clean column names to snake_case
  pivot_longer(cols = starts_with("series"),             # pivot the data from wide to long format
               names_to = "series", 
               names_prefix = "series_",
               values_to = "viewership") 

# show the first 10 rows of this dataset. 
print(viewers, n = 10)

# what was the average viewership in Season 1? In Season 5?
avg_viewership <- viewers |> 
  filter(series %in% c(1, 5)) |> 
  group_by(series) |> 
  summarise(avg_viewership = mean(viewership, 
                                  na.rm = TRUE))

avg_viewership

```
The average viewship of Season 1 is 2.7700 and that of Season 5 is 10.0393.
